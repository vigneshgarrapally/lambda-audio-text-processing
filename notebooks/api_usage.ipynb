{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as a quick reference for using the mentioned third party APIs. Follow along with the examples to get a better understanding of how to use the APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transcription API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the audio-to-text transcription, we will go through two APIs namely Open AI Whisper and Deepgram Nova.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Open AI Whisper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File types Supported : `mp3`,`mp4`,`mpeg`,`mpga`,`4a`,`wav` and `webm`. \\\n",
    "File Size Limit : 25 MB \\\n",
    "Operations Supported by Model : `transcription` and `translate` \\\n",
    "Languages Supported : [Click here](https://platform.openai.com/docs/guides/speech-to-text/supported-languages) \\\n",
    "Prompt can be sent optionally that will be sent to GPT to improve the accuracy of the transcription and correct any mistakes in the transcription.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dynamic workshop aims to provide up-to-date information on pharmacological approaches, issues, and treatment in the geriatric population to assist in preventing medication-related problems, appropriately and effectively managing medications and compliance. The concept of polypharmacy, taking multiple types of drugs, will also be discussed, as this is a common issue that can impact adverse side effects in the geriatric population. Participants will leave with the knowledge and considerations of common drug interaction and how to minimize effects that limit function. Summit Professional Education is approved provider of continuing education. This course is offered for six units. This course contains content classified under both the domain of occupational therapy and professional issues, period.\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "audio_file = open(\"../inputs/sample_audio.wav\", \"rb\")\n",
    "# returns a JSON Object\n",
    "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "openai_output = transcript.text\n",
    "print(openai_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Deepgram Nova\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of Transcription : `live` and `pre-recorded` \\\n",
    "Features Supported : [Features](https://developers.deepgram.com/docs/features-overview). \\\n",
    "Models Supported : [Models](https://developers.deepgram.com/docs/models-overview)\n",
    "These features can be sent as an options in the form of a dict to the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dynamic workshop aims to provide up to date information on pharmacological approach as comma issues, comma and treatment in the geriatric population to assist in preventing medication related problems, comma appropriately and effectively managing medications and compliance period. The concept of poly pharmacy parenthesis taking multiple types of drugs parenthesis will also be discussed as this is a common issue that can impact adverse side effects in the aeriatric population period. Participants will leave with the knowledge and considerations of common drug interaction and how to minimize the effects that limit function, period. Summit professional education is approved provider of continuing education Perion. This course is offered for six units. Period. This course contains content classified under the both the domain of patiental therapy and professional issues, period.\n"
     ]
    }
   ],
   "source": [
    "from deepgram import Deepgram\n",
    "import os\n",
    "import json\n",
    "\n",
    "deepgram_api_key = os.getenv(\"DEEPGRAM_API_KEY\")\n",
    "dg_client = Deepgram(deepgram_api_key)\n",
    "\n",
    "options = {\"punctuate\": True, \"model\": \"general\", \"tier\": \"enhanced\"}\n",
    "\n",
    "with open(\"../inputs/sample_audio.wav\", \"rb\") as audio:\n",
    "    source = {\"buffer\": audio, \"mimetype\": \"audio/wav\"}\n",
    "    # you can aslo send direct url of the audio file as the source\n",
    "    response = await dg_client.transcription.prerecorded(source, options)\n",
    "    deepgram_output = response[\"results\"][\"channels\"][0][\"alternatives\"][0][\n",
    "        \"transcript\"\n",
    "    ]\n",
    "    print(deepgram_output)\n",
    "    # save the response as json in the output folder with audio file name as the name of the json file\n",
    "    with open(\"../outputs/sample_audio.json\", \"w\") as f:\n",
    "        json.dump(response, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text-to-Speech API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert text to speech, we will use following two APIs namely Google Cloud Text-to-Speech and ElevenLabs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 ElevenLabs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models available : https://docs.elevenlabs.io/speech-synthesis/models \\\n",
    "Prompting : https://docs.elevenlabs.io/speech-synthesis/prompting \\\n",
    "Voice : https://docs.elevenlabs.io/voicelab/overview \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elevenlabs import generate, play, set_api_key\n",
    "\n",
    "set_api_key(os.getenv(\"ELEVENLABS_API_KEY\"))\n",
    "\n",
    "audio = generate(\n",
    "    text=openai_output,\n",
    "    voice=\"Bella\",\n",
    "    model=\"eleven_monolingual_v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the audio file\n",
    "with open(\"../outputs/elevenlabs_sample_audio.wav\", \"wb\") as f:\n",
    "    f.write(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Google Cloud Text-to-Speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- Languages: 57 -----------------------\n",
      "     af-ZA     ar-XA     bg-BG     bn-IN     ca-ES\n",
      "    cmn-CN    cmn-TW     cs-CZ     da-DK     de-DE\n",
      "     el-GR     en-AU     en-GB     en-IN     en-US\n",
      "     es-ES     es-US     eu-ES     fi-FI    fil-PH\n",
      "     fr-CA     fr-FR     gl-ES     gu-IN     he-IL\n",
      "     hi-IN     hu-HU     id-ID     is-IS     it-IT\n",
      "     ja-JP     kn-IN     ko-KR     lt-LT     lv-LV\n",
      "     ml-IN     mr-IN     ms-MY     nb-NO     nl-BE\n",
      "     nl-NL     pa-IN     pl-PL     pt-BR     pt-PT\n",
      "     ro-RO     ru-RU     sk-SK     sr-RS     sv-SE\n",
      "     ta-IN     te-IN     th-TH     tr-TR     uk-UA\n",
      "     vi-VN    yue-HK"
     ]
    }
   ],
   "source": [
    "import google.cloud.texttospeech as tts\n",
    "from typing import Sequence\n",
    "\n",
    "api_key_string = os.getenv(\"GOOGLE_CLOUD_TEXT_TO_SPEECH_API_KEY\")\n",
    "\n",
    "\n",
    "def unique_languages_from_voices(voices: Sequence[tts.Voice]):\n",
    "    language_set = set()\n",
    "    for voice in voices:\n",
    "        for language_code in voice.language_codes:\n",
    "            language_set.add(language_code)\n",
    "    return language_set\n",
    "\n",
    "\n",
    "def list_languages():\n",
    "    client = tts.TextToSpeechClient(client_options={\"api_key\": api_key_string})\n",
    "    response = client.list_voices()\n",
    "    languages = unique_languages_from_voices(response.voices)\n",
    "\n",
    "    print(f\" Languages: {len(languages)} \".center(60, \"-\"))\n",
    "    for i, language in enumerate(sorted(languages)):\n",
    "        print(f\"{language:>10}\", end=\"\\n\" if i % 5 == 4 else \"\")\n",
    "\n",
    "\n",
    "list_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.texttospeech as tts\n",
    "\n",
    "\n",
    "def text_to_wav(voice_name: str, text: str):\n",
    "    language_code = \"-\".join(voice_name.split(\"-\")[:2])\n",
    "    text_input = tts.SynthesisInput(text=text)\n",
    "    voice_params = tts.VoiceSelectionParams(\n",
    "        language_code=language_code, name=voice_name\n",
    "    )\n",
    "    audio_config = tts.AudioConfig(audio_encoding=tts.AudioEncoding.LINEAR16)\n",
    "\n",
    "    client = tts.TextToSpeechClient(client_options={\"api_key\": api_key_string})\n",
    "    response = client.synthesize_speech(\n",
    "        input=text_input,\n",
    "        voice=voice_params,\n",
    "        audio_config=audio_config,\n",
    "    )\n",
    "\n",
    "    filename = f\"../outputs/{voice_name}.wav\"\n",
    "    with open(filename, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "        print(f'Generated speech saved to \"{filename}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated speech saved to \"../outputs/en-AU-Neural2-A.wav\"\n",
      "Generated speech saved to \"../outputs/en-GB-Neural2-B.wav\"\n",
      "Generated speech saved to \"../outputs/en-IN-Wavenet-C.wav\"\n",
      "Generated speech saved to \"../outputs/en-US-Studio-O.wav\"\n",
      "Generated speech saved to \"../outputs/fr-FR-Neural2-A.wav\"\n",
      "Generated speech saved to \"../outputs/fr-CA-Neural2-B.wav\"\n"
     ]
    }
   ],
   "source": [
    "text_to_wav(\"en-AU-Neural2-A\", \"What is the temperature in Sydney?\")\n",
    "text_to_wav(\"en-GB-Neural2-B\", \"What is the temperature in London?\")\n",
    "text_to_wav(\"en-IN-Wavenet-C\", \"What is the temperature in Delhi?\")\n",
    "text_to_wav(\"en-US-Studio-O\", \"What is the temperature in New York?\")\n",
    "text_to_wav(\"fr-FR-Neural2-A\", \"Quelle est la température à Paris ?\")\n",
    "text_to_wav(\"fr-CA-Neural2-B\", \"Quelle est la température à Montréal ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LangChain and OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"prompt\"],\n",
    "    template=\"Prompt that was provided is: {prompt}.Text that was provided is: {text}.\",\n",
    ")\n",
    "prompt.format(\n",
    "    text=\"I want to start a company that makes cars.\",\n",
    "    prompt=\"Suggest a name for my company.\",\n",
    ")\n",
    "llm = OpenAI(temperature=0)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "llm_chain.predict(\n",
    "    text=\"I want to start a company that makes cars.\",\n",
    "    prompt=\"Suggest a name for my company.\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
